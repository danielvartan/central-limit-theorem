[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Central Limit Theorem",
    "section": "Overview",
    "text": "Overview\nThe aim of this analysis is to illustrate the the Law of Large numbers (LLN) and the Central Limit Theorem (CLT).\nPlease note that this is just an exercise and some documentation may be missing."
  },
  {
    "objectID": "index.html#two-theorems-regarding-large-random-samples-lln-clt",
    "href": "index.html#two-theorems-regarding-large-random-samples-lln-clt",
    "title": "Central Limit Theorem",
    "section": "Two theorems regarding large random samples: LLN & CLT",
    "text": "Two theorems regarding large random samples: LLN & CLT\nTo get a grasp of the Central Limit Theorem (CLT), it’s important to also understand the Law of Large numbers (LLN). Combined, they allow us to make inferences regardless of the original distribution of the population.\nThe LLN is maybe the easier of the two to understand. It states that the sample mean will converge to the population mean as the sample size increases.\nHere is the theorem regarding the LLN, as described in DeGroot & Schervish (2012, p. 352).\n\nTheorem 1 (Law of Large Numbers) Suppose that \\(X_{1}, \\dots, X_{n}\\) form a random sample from a distribution for which the mean is \\(\\mu\\) and for which the variance is finite. Let \\(\\overline{X}_{n}\\) denote the sample mean. Then:\n\\[\n\\overline{X}_{n} \\xrightarrow{p} \\mu\n\\tag{1}\\]\n\nThe term “central limit theorem” was likely coined by Georg Pólya in 1920 (Pólya, 1920), referring to a fundamental concept in probability theory. This theorem, initially discovered by Laplace in 1810, explains the frequent appearance of the Gaussian probability density (\\(e^{-x^{2}}\\)) in various situations. Today, the term encompasses a wide range of statements concerning the convergence of probability distributions to a normal distribution, applicable to functions of various random variables or elements (Fischer, 2011).\nHere is the main theorem regarding the CLT, as described by DeGroot & Schervish (2012, p. 361).\n\nTheorem 2 (Central Limit Theorem (Lindeberg and Lévy)) If the random variables \\(X_{1}, \\dots, X_{n}\\) form a random sample of size \\(n\\) from a given distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) (\\(0 &lt; \\sigma^{2} &lt; \\infty\\)), then for each fixed number \\(x\\),\n\\[\n\\lim_{n \\to \\infty} \\Pr \\left[ \\frac{\\overline{X}_{n} - \\mu}{\\sigma / \\sqrt{n}} \\le x \\right] = \\Phi(x),\n\\tag{2}\\]\nwhere \\(\\Phi\\) denotes the cumulative distribution function (c.d.f.) of the standard normal distribution.\n\n\nThe interpretation of Equation 2 is as follows: If a large random sample is taken from any distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), regardless of whether this distribution is discrete or continuous, then the distribution of the random variable \\(n^{1/2}(\\overline{X}_n - \\mu) / \\sigma\\) will be approximately the standard normal distribution. Therefore, the distribution of \\(\\overline{X}_{n}\\) will be approximately the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}/n\\), or, equivalently, the distribution of the sum \\(\\sum^{n}_{i = 1} X_{i}\\) will be approximately the normal distribution with mean \\(n \\mu\\) and variance \\(n \\sigma^{2}\\). (DeGroot & Schervish, 2012, pp. 361–362)\n\nIn the next sections, you will see these theorems in action."
  },
  {
    "objectID": "index.html#simulating-the-central-limit-theorem-clt",
    "href": "index.html#simulating-the-central-limit-theorem-clt",
    "title": "Central Limit Theorem",
    "section": "Simulating the Central Limit Theorem (CLT)",
    "text": "Simulating the Central Limit Theorem (CLT)\nLet’s first run some code to set up the environment.\n\nCodelibrary(dplyr, quietly = TRUE)\nlibrary(checkmate, quietly = TRUE)\nlibrary(gifski, quietly = TRUE)\nlibrary(gganimate, quietly = TRUE)\nlibrary(ggplot2, quietly = TRUE)\nlibrary(latex2exp, quietly = TRUE)\nlibrary(magick, quietly = TRUE)\nlibrary(purrr, quietly = TRUE)\nlibrary(rutils, quietly = TRUE) # https://github.com/danielvartan/rutils\nlibrary(stats, quietly = TRUE)\n\n\n\nCoden_args &lt;- function(fun) {\n  checkmate::assert_function(fun)\n\n  as.list(args(fun)) %&gt;%\n    `[`(-length(.)) |&gt;\n    names() |&gt;\n    length()\n}\n\nargs_names &lt;- function(fun) {\n  checkmate::assert_function(fun)\n\n  as.list(args(fun)) %&gt;%\n    `[`(-length(.)) |&gt;\n    names()\n}\n\nassert_function &lt;- function(fun, ...) {\n  checkmate::assert_function(fun)\n\n  args &lt;- list(...)\n  n_args &lt;- n_args(fun)\n\n  if (n_args == 0) {\n    cli::cli_abort(paste0(\n      \"{.strong {cli::col_red('fun')}} must have at least one parameter.\"\n    ))\n  }\n\n  unassigned_parameters &lt;-\n    formals(fun) %&gt;% # Don't change the pipe\n    `[`(-1) |&gt;\n    purrr::map(~ ifelse(inherits(.x, \"name\"), TRUE, FALSE)) |&gt;\n    purrr::keep(~ isTRUE(.x)) |&gt;\n    names()\n\n  if (any(!unassigned_parameters %in% names(args), na.rm = TRUE)) {\n    unassigned_minus_arg &lt;-\n      unassigned_parameters %&gt;%\n      `[`(!unassigned_parameters %in% names(args))\n\n    cli::cli_abort(paste0(\n      \"{.strong {cli::col_blue('fun')}} \",\n      \"{cli::qty(length(unassigned_minus_arg))} parameter{?s} \",\n      \"{.strong {unassigned_minus_arg}} must be assigned in \",\n      \"{.strong {cli::col_red('...')}} (ellipsis).\"\n    ))\n  }\n\n  invisible(NULL)\n}\n\n\n\nCodeplot_dist &lt;- function(\n    fun = stats::dnorm,\n    ...,\n    n = 1000,\n    geom = \"function\",\n    x_lim = c(-5, 5),\n    y_lim = NULL,\n    title = NULL,\n    subtitle = NULL\n  ) {\n  assert_function(fun, ...)\n  checkmate::assert_integerish(n, lower = 1)\n  checkmate::assert_string(geom)\n  checkmate::assert_numeric(x_lim, len = 2)\n  checkmate::assert_numeric(y_lim, len = 2, null.ok = TRUE)\n  checkmate::assert_multi_class(\n    title, c(\"character\", \"latexexpression\"), null.ok = TRUE\n  )\n  checkmate::assert_multi_class(\n    subtitle, c(\"character\", \"latexexpression\"), null.ok = TRUE\n  )\n\n  if (is.character(title)) checkmate::assert_string(title, null.ok = TRUE)\n  if (is.character(subtitle)) checkmate::assert_string(subtitle, null.ok = TRUE)\n\n  args &lt;- list(...)\n\n  plot &lt;- ggplot2::ggplot() +\n    ggplot2::stat_function(\n      fun = fun,\n      args = args,\n      n = n,\n      geom = geom,\n      colour = \"red\",\n      linewidth = 1\n    ) +\n    ggplot2::xlim(x_lim[1], x_lim[2]) +\n    ggplot2::labs(y = \"Density\")\n\n  if (!is.null(y_lim)) plot &lt;- plot + ggplot2::ylim(y_lim[1], y_lim[2])\n  if (!is.null(title)) plot &lt;- plot + ggplot2::labs(title = title)\n  if (!is.null(subtitle)) plot &lt;- plot + ggplot2::labs(subtitle = subtitle)\n\n  print(plot)\n  invisible(plot)\n}\n\n\n\nCodeclt_data &lt;- function(\n    fun = stats::runif,\n    ...,\n    sample_size = 10, # Number of observations\n    sample_n_max = 150, # Maximum Number of samples\n    dist_name = NULL\n  ) {\n  assert_function(fun, ...)\n  checkmate::assert_number(sample_size, lower = 1)\n  checkmate::assert_number(sample_n_max, lower = 1)\n  checkmate::assert_string(dist_name, null.ok = TRUE)\n\n  args &lt;- list(...)\n\n  if (length(args) == 0 || !args_names(fun)[1] %in% names(args)) {\n    args[args_names(fun)[1]] &lt;- sample_size\n  }\n\n  distribution &lt;- ifelse(\n    is.null(dist_name),\n    deparse(substitute(fun)),\n    dist_name\n  )\n\n  data &lt;- purrr::map(\n    seq(1, sample_n_max),\n    ~ dplyr::tibble(\n      distribution = distribution,\n      sample_size = sample_size,\n      sample_number = .x,\n      sample_mean = do.call(fun, args = args) |&gt; mean()\n    )\n  ) |&gt;\n    purrr::reduce(dplyr::add_row)\n\n  purrr::map(\n    seq(1, sample_n_max),\n    ~ data |&gt;\n      dplyr::slice(seq_len(.x)) |&gt;\n      dplyr::mutate(group = .x)\n  ) |&gt;\n    purrr::reduce(dplyr::add_row) |&gt;\n    dplyr::mutate(group = as.factor(group))\n}\n\nplot_clt &lt;- function(\n    fun = stats::runif,\n    ...,\n    sample_size = 10, # Number of observations\n    sample_n_max = 150, # Maximum Number of samples\n    title = \"Central Limit Theorem\",\n    dist_name = NULL\n  ) {\n  assert_function(fun, ...)\n  checkmate::assert_number(sample_size, lower = 1)\n  checkmate::assert_number(sample_n_max, lower = 1)\n  checkmate::assert_string(title, null.ok = TRUE)\n  checkmate::assert_multi_class(\n    title, c(\"character\", \"latexexpression\"), null.ok = TRUE\n  )\n  checkmate::assert_string(dist_name, null.ok = TRUE)\n\n  data &lt;- clt_data(\n    fun = fun,\n    ...,\n    sample_size = sample_size,\n    sample_n_max = sample_n_max,\n    dist_name = dist_name\n  )\n\n  y_plot &lt;-\n    data |&gt;\n    dplyr::filter(group == sample_n_max) %&gt;% {\n      ggplot2::ggplot(., ggplot2::aes(x = sample_mean)) +\n        ggplot2::stat_function(\n          fun = stats::dnorm,\n          args = list(\n            mean = .$sample_mean |&gt; mean(),\n            sd = .$sample_mean |&gt; stats::sd()\n          )\n        )\n    }\n\n  y_lim &lt;-\n    (ggplot2::layer_scales(y_plot)$y$range$range[2]) %&gt;%\n    `*`(3 / 2) %&gt;%\n    c(0, .)\n\n  plot &lt;-\n    data %&gt;% {\n      ggplot2::ggplot(., ggplot2::aes(x = sample_mean)) +\n        ggplot2::geom_histogram(\n          ggplot2::aes(y = ggplot2::after_stat(density)),\n          colour = \"black\",\n          fill = \"white\",\n          bins = 30\n        ) +\n        ggplot2::stat_function(\n          fun = stats::dnorm,\n          args = list(\n            mean = .$sample_mean |&gt; mean(),\n            sd = .$sample_mean |&gt; stats::sd()\n          ),\n          n = 1000,\n          colour = \"red\",\n          linewidth = 1,\n          xlim = c(\n            mean(.$sample_mean) - (5 * stats::sd(.$sample_mean)),\n            mean(.$sample_mean) + (5 * stats::sd(.$sample_mean))\n          )\n        ) +\n        ggplot2::ylim(y_lim) +\n        ggplot2::labs(x = \"Sample mean\", y = \"Density\") +\n        ggplot2::theme(text = ggplot2::element_text(size = 12.5))\n    }\n\n  if (!is.null(title)) plot &lt;- plot + ggplot2::labs(title = title)\n\n  anim &lt;-\n    plot +\n    gganimate::transition_states(group) +\n    gganimate::ease_aes(\"exponential-out\") +\n    ggplot2::labs(subtitle = paste0(\n      ifelse(\n        !is.null(dist_name),\n        dist_name,\n        deparse(substitute(fun))\n      ), \" | \",\n      \"Sample size = \", sample_size, \" | \",\n      \"Sample number = {closest_state}\"\n    ))\n\n  anim |&gt;\n    gganimate::animate(\n      renderer = gganimate::gifski_renderer(),\n      ref_frame = -1\n    ) |&gt;\n    suppressWarnings()\n}\n\n\nNow, let’s set the parameters of the simulation.\n\nCodesample_size &lt;- 30 # Number of observations for each sample\nsample_n_max &lt;- 300 # Maximum number of samples\n\n\nIn the next sections, we will draw samples from different distributions and see how the Central Limit Theorem works.\nDrawing from the uniform distribution\n\n\nCodedist_name &lt;- \"Uniform distribution\"\n\n\n\nCodeplot_dist(\n  fun = stats::dunif,\n  x_lim = c(0, 1),\n  y_lim = c(0, 1.5),\n  title = dist_name\n)\n\n\n\n\n\n\n\n\nCodeplot_clt(\n  fun = stats::runif,\n  sample_size = sample_size,\n  sample_n_max = sample_n_max,\n  dist_name = dist_name\n)\n\n\n\n\n\n\n\nIt looks like the sample means are normally distributed, as expected by the Central Limit Theorem. Let’s see if this holds for other distributions."
  },
  {
    "objectID": "index.html#drawing-from-the-binomial-distribution",
    "href": "index.html#drawing-from-the-binomial-distribution",
    "title": "Central Limit Theorem",
    "section": "Drawing from the binomial distribution\n",
    "text": "Drawing from the binomial distribution\n\n\nCoden &lt;- 40\ndist_name = \"Binomial distribution\"\n\n\n\nCodeplot_dist(\n  fun = stats::dbinom,\n  x = seq(1, n, by = 1),\n  size = n,\n  prob = 0.5,\n  n = n,\n  geom = \"point\",\n  x_lim = c(0, n),\n  title = dist_name,\n  subtitle = latex2exp::TeX(paste0(\"$n = \", n, \"$ | $p = 0.5$\"))\n) |&gt;\n  rutils:::shush()\n\n\n\n\n\n\n\n\nCodeplot_clt(\n  fun = stats::rbinom,\n  size = 100, # Number of trials\n  prob = 0.5, # Probability of success on each trial\n  sample_size = sample_size, # Number of observations\n  sample_n_max = sample_n_max,\n  dist_name = dist_name\n)\n\n\n\n\n\n\n\nSame here."
  },
  {
    "objectID": "index.html#drawing-from-the-beta-distribution",
    "href": "index.html#drawing-from-the-beta-distribution",
    "title": "Central Limit Theorem",
    "section": "Drawing from the beta distribution\n",
    "text": "Drawing from the beta distribution\n\n\nCodedist_name = \"Beta distribution\"\n\n\n\nCodeplot_dist(\n  fun = stats::dbeta,\n  shape1 = 2,\n  shape2 = 5,\n  ncp = 0,\n  x_lim = c(0, 1),\n  title = \"Beta distribution\",\n  subtitle = latex2exp::TeX(\"$\\\\alpha = 2$ | $\\\\beta = 5$\")\n)\n\n\n\n\n\n\n\n\nCodeplot_clt(\n  fun = stats::rbeta,\n  shape1 = 2,\n  shape2 = 5,\n  ncp = 0,\n  sample_size = sample_size,\n  sample_n_max = sample_n_max,\n  dist_name = dist_name\n)\n\n\n\n\n\n\n\nSame. Again."
  },
  {
    "objectID": "index.html#drawing-from-the-cauchy-distribution",
    "href": "index.html#drawing-from-the-cauchy-distribution",
    "title": "Central Limit Theorem",
    "section": "Drawing from the Cauchy distribution\n",
    "text": "Drawing from the Cauchy distribution\n\nHere is a tricky one for the Central Limit Theorem.\nThe Cauchy distribution violates one of the conditions of the CLT, which is that the population must have a finite variance (\\(0 &lt; \\sigma^{2} &lt; \\infty\\)). This distribution has an infinite variance.\nLet’s see what happens when we draw samples from it.\n\nCodedist_name = \"Cauchy distribution\"\n\n\n\nCodeplot_dist(\n  fun = stats::dcauchy,\n  location = 1,\n  scale = 0.25,\n  x_lim = c(-0.5, 2.5),\n  title = \"Cauchy distribution\",\n  subtitle = latex2exp::TeX(\"$x_{0} = 1$ | $\\\\gamma = 0.25$\")\n)\n\n\n\n\n\n\n\n\nCodeplot_clt(\n  fun = stats::rcauchy,\n  location = 1,\n  scale = 0.25,\n  sample_size = sample_size,\n  sample_n_max = sample_n_max,\n  dist_name = dist_name\n)\n\n\n\n\n\n\n\nAs expected, the sample means are not normally distributed. This is because the Cauchy distribution has an infinite variance, which violates one of the conditions of the Central Limit Theorem."
  },
  {
    "objectID": "index.html#making-sense-of-the-central-limit-theorem-with-the-law-of-large-numbers",
    "href": "index.html#making-sense-of-the-central-limit-theorem-with-the-law-of-large-numbers",
    "title": "Central Limit Theorem",
    "section": "Making sense of the Central Limit Theorem with the Law of Large Numbers",
    "text": "Making sense of the Central Limit Theorem with the Law of Large Numbers\nThe Central Limit Theorem (CLT) states that the distribution of the sample means will be approximately normal, regardless of the original distribution of the population* (*with a few restrictions). This means that if you take multiple samples from a population and calculate their means, the distribution of those means will tend to be normal.\nThe Law of Large Numbers (LLN) complements the CLT by stating that as the sample size increases, the sample mean will converge to the population mean. In other words, the more observations you have, the more likely it is that your sample mean will accurately reflect the true population mean. This is because the probability to get an extreme value decreases as the sample size increases.\nLet’s do a test to see if this holds true. We will draw samples from a normal distribution and calculate the sample mean. We will do this 100 times for a given sample size and then calculate the mean of the sample means to get a representative mean for the sample size.\n\nCodeplot_dist(\n  fun = stats::dnorm,\n  mean = 0,\n  sd = 1,\n  title = \"Normal distribution\",\n  subtitle = latex2exp::TeX(\"$\\\\mu = 0$ | $\\\\sigma = 1$\")\n)\n\n\n\n\n\n\n\n\nCodemean_test &lt;- function(sample_size, iterations = 100) {\n  purrr::map(\n    seq_len(iterations),\n    ~ rnorm(sample_size, mean = 0, sd = 1) |&gt; mean()\n  ) |&gt;\n  purrr::reduce(mean)\n}\n\n\n\nCodex_max &lt;- 1000\n\ndata &lt;- dplyr::tibble(\n  x = seq(1, x_max, by = 1),\n  y = purrr::map_dbl(x, mean_test) |&gt; unlist()\n)\n\n\nLet’s now plot the sample mean as a function of the sample size.\n\nCodedata |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n  ggplot2::geom_line() +\n  ggplot2::geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  ggplot2::geom_vline(xintercept = 30, linetype = \"dashed\", color = \"blue\") +\n  ggplot2::scale_x_continuous(breaks = sort(c(30, seq(0, x_max, by = 100)))) +\n  ggplot2::labs(x = \"Sample size\", y = \"Sample mean\")\n\n\n\n\n\n\n\nAs you can see, the sample mean is close to the population mean (red dashed line) with a sample size of around 30 (blue dashed line). 30 is just a rule of thumb, like a minimum size to the CLT to kick in, but it’s important to always do a power analysis when estimating sample sizes (see one here!). The sample mean will be even closer to the population mean as the sample size increases.\nConsidering that the sample mean comes from a normal distribution (as seen in the CLT), and that, as we have more and more observations, the sample mean will converge to the population mean (as seen in the LLN), we can say that the sample mean can be an unbiased estimator of the population mean.\nUnconvinced? Let’s try this from another angle.\n\nCodegg_color_hue &lt;- function(n) {\n  hues = seq(15, 375, length = n + 1)\n  hcl(h = hues, l = 65, c = 100)[seq(1, n)]\n}\n\npull_mean &lt;- function(data, s_size) {\n  data |&gt;\n    dplyr::filter(sample_size == s_size) |&gt;\n    dplyr::pull(mean) %&gt;% `[`(1)\n}\n\n\n\nCodedata &lt;-\n  purrr::map(\n    seq(30, 150, by = 30),\n    ~ dplyr::tibble(\n      sample_size = .x,\n      x = rnorm(.x, mean = 0, sd = 1),\n      mean = x |&gt; mean()\n    )\n  ) |&gt;\n    purrr::reduce(dplyr::bind_rows) |&gt;\n  dplyr::mutate(sample_size = as.factor(sample_size))\n\n\nHere we have a plot of the sample means for different sample sizes. The dashed lines represent the population mean and the sample means for each sample size.\n\nCodeplot &lt;- data |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = x, color = sample_size)) +\n  ggplot2::stat_function(\n    fun = stats::dnorm,\n    args = list(mean = 0, sd = 1),\n    n = 1000,\n    colour = \"black\",\n    linewidth = 1\n  ) +\n  ggplot2::geom_line(stat = \"density\", alpha = 0.5, linewidth = 0.75) +\n  ggplot2::geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +\n  ggplot2::xlim(-5, 5) +\n  ggplot2::labs(x = \"Sample mean\", y = \"Density\", color = \"Sample size\") +\n  ggplot2::scale_color_manual(\n    values = c(\"#F8766D\", \"#A3A500\", \"#00BF7D\", \"#00B0F6\", \"#E76BF3\")\n  )\n\nfor (i in seq_along(unique(data$sample_size))) {\n  plot &lt;-\n    plot +\n    ggplot2::geom_vline(\n      xintercept = pull_mean(data, unique(data$sample_size)[i]),\n      linetype = \"dashed\",\n      color = gg_color_hue(length(unique(data$sample_size)))[i],\n      alpha = 0.5\n    )\n}\n\nplot\n\n\n\n\n\n\n\nSee how the sample means (dashed lines) are getting closer to the population mean (black dashed line) as the sample size increases? That’s the Law of Large Numbers in action.\nOther materials\nI hope this explanations, visualizations and code have helped you understand the Central Limit Theorem. If you want to learn more about it, I recommend the following videos:\n\nStatQuest with Josh Starmer: The Central Limit Theorem, clearly explained!!!\n\n3Blue1Brown: But what is the Central Limit Theorem?\n\nVery normal: Understand the normal distribution and Central Limit Theorem\n\n\nDon’t forget to check the references below for more information."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Central Limit Theorem",
    "section": "References",
    "text": "References\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (4th ed.). Addison-Wesley.\n\n\nFischer, H. (2011). A history of the central limit theorem: From classical to modern probability theory. Springer New York. https://doi.org/10.1007/978-0-387-87857-7\n\n\nPólya, G. (1920). Über den zentralen grenzwertsatz der wahrscheinlichkeitsrechnung und das momentenproblem. Mathematische Zeitschrift, 8, 171–181. https://doi.org/10.1007/BF01206525"
  }
]